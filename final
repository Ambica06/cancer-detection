pip install pip-autoremove

pip-autoremove pandas -y

pip install pandas

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded=files.upload()

df=pd.read_csv("data.csv")
df.head(7)

#count tbe number of rows and columns in the dataset
df.shape #569 rows(training examples) 33 col (features)

#count the number of empty(NaN, NAN, na) values in each column
df.isna().sum()

#drop the column with all missing values
df=df.dropna(axis=1)

#new count of no o rows and col
df.shape

#get count of malignant(M) or benign(B) cells
df['diagnosis'].value_counts()

#visualize the count
sns.countplot(df['diagnosis'], label='count')

#look at the datatypes to see which columns need to be encoded
df.dtypes

#encode the categorical data values
from sklearn.preprocessing import LabelEncoder
labelencoder_Y=LabelEncoder()
df.iloc[:, 1] = labelencoder_Y.fit_transform(df.iloc[:,1].values)
df.iloc[:, 1]

#create a pair plot
sns.pairplot(df.iloc[:, 1:5], hue='diagnosis')

#print first five rows of new data
df.head(5)

#get the correlations of the columns
df.iloc[:, 1:12].corr()

#visualize the correlation
plt.figure(figsize=(10,10))
sns.heatmap(df.iloc[:, 1:12].corr(), annot=True, fmt='.0%')

#split the dataset into independent(X) and dependent(Y) datasets
X=df.iloc[:, 2:31].values
Y=df.iloc[:, 1].values

#split the dataset into training and test cases
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)

#feature scaling
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

#create a function for the models
def models(X_train, Y_train) :
  #logistic Regression model
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state=0)
  log.fit(X_train, Y_train)

  #decision tree
  from sklearn.tree import DecisionTreeClassifier
  tree=DecisionTreeClassifier(criterion = 'entropy', random_state=0)
  tree.fit(X_train, Y_train)

  #random forest classifier
  from sklearn.ensemble import RandomForestClassifier
  forest=RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
  forest.fit(X_train, Y_train)

  #print the models accuracy on the training data
  print('[0]LR Training accuracy:', log.score(X_train, Y_train))
  print('[1]Decision Tree Training accuracy:', tree.score(X_train, Y_train))
  print('[2]Random Forest Training accuracy:', forest.score(X_train, Y_train))
  return log, tree, forest

model = models(X_train, Y_train)

#test model accuracy confusion matrix which shows true/false positives/negatives
from sklearn.metrics import confusion_matrix
for i in range ( len(model) ) :
  print('Model ', i)
  cm = confusion_matrix(Y_test, model[i].predict(X_test))
  TP = cm[0][0]
  TN = cm[1][1]
  FN = cm[1][0]
  FP = cm[0][1]
  print(cm)
  print('Testing accuracy :', (TP+TN)/(TP+TN+FP+FN))
  print()

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
for i in range( len(model) ):
  print('Model ', i)
  print( classification_report(Y_test, model[i].predict(X_test)))
  print (accuracy_score(Y_test, model[i].predict(X_test)))
  print()

#print the prediction of forest classifier model
pred=model[2].predict(X_test)
print(pred)
print()
print(Y_test)
